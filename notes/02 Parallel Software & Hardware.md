# 02 Parallel Software & Hardware
## 1. 背景知识
1. 冯诺依曼结构：包括主存、CPU、以及主存和CPU之间的互连结构
   1. CPU分为控制单元和算术逻辑单元（ALU）
   2. 冯诺依曼瓶颈：主存和CPU之间的分离，由于CPU执行指令的速度远超主存取指速度，导致冯诺依曼体系结构效率的瓶颈
2. 进程、多任务以及线程
   1. 进程：运行的程序
   2. 多任务：操作系统提供对同时运行多个程序的支持
   3. 线程：线程为程序员提供了一种机制，将程序划分为多个大致独立的任务，当某个任务阻塞时能执行其他任务。此外，在大多数系统中，线程间的切换比进程间的切换更快。因为线程相对于进程而言是“轻量级”的。线程包含在进程中，所以线程可以使用相同的可执行代码，共享相同的内存和相同的 I/O 设备。实际上，当两个线程共属于一个进程时，它们共享进程的大多数资源。它们之间最大的差别是各自需要一个私有的程序计数器和函数调用栈，使它们能够独立运行。

## 2. 对冯诺依曼模型的改进
1. Cache：对高速缓冲存储器 (cache，简称缓存) 的访问时间比其他存储区域的访问时间短。CPU Cache 是一组相比于主存，CPU能更快速地访问的内存区域
   1. 局部性原理：程序访问完一个存储区域往往会访问接下来的区域；在访问完一个内存区域，程序会在不久的将来（时间局部性）访问临近的区域（空间局部性）
   2. 命中和缺失：当向Cache 查询信息时，如果 Cache 中有信息，则称为 Cache 命中或者命中；如果信息不存在，则称为 Cache 缺失或者缺失
   3. 写直达和写回：在写直达 （write-through） Cache 中，当 CPU 向Cache 写数据时，高速缓存行会立即写入主存中。在写回 （write-back）Cache 中，数据不是立即更新到主存中，而是将发生数据更新的高速缓存行标记成脏 （dirty）。当发生高速缓存行替换时，标记为脏的高速缓存行被写入主存中
   4. Cache映射：一个极端是全相联Cache, 每个高速缓存行能够放置在 Cache 中的任意位置。另一个极端是直接映射Cache, 每个高速缓存行在 Cache 中有唯一的位置。处于两种极端中间的方案是 n 路组相联。在 n 路组相联 Cache 中，每个高速缓存行都能放置到 Cache 中几个不同区域位置中的一个
   5. Cache替换或驱逐：最常用的替换方案是最近最 少使用 （least recently used）。顾名思义，Cache 记录各个块被访问的次数，替换最近访问次数最少的块
2. 虚拟存储器
   1. 如果运行一个大型的程序，或者程序需要访问大型数据集，那么所有的指令或者数据可能在主存中放不下，利用虚拟存储器 (或虚拟内存)，使得主存可以作为辅存的缓存。它通过在主存中存放当前 执行程序所需要用到的部分，来利用时间和空间局部性；那些暂时用不到的部分存储在辅存的块，称为交换空间 (swap space) 中
   2. 页：虚拟存储器也是对数据块和指令块进行操作。这些块通常称为页 (page)
   3. 页表：当程序运行时，创建一张将虚拟页号映射成物理地址的表。程序运行时使用到虚拟地址，这个页表就用来将虚拟地址转换成物理地址
   4. TLB：TLB 在快速存储介质中缓存了一些页表的条目。利用时间和空间局部性原理，大部分存储器所访问页的物理地址已经存储在 TLB 中，对主存中页表的访问能够大幅度减少
3. 指令级并行 ILP：通过让多个处理器部件或者功能单元同时执行指令来提高处理器的性能。有两种主要方法来实现指令级并行：**流水线**和**多发射**。流水线是指将功能单元分阶段安排；多发射是指让多条指令同时启动
   1. 流水线
   2. 多发射：如果功能单元是在编译时调度的，则称该多发射系统使用静态多发射；如果是在运行时间调度的，则称该多发射系统使用动态多发射。一个支持动态多发射的处理器称为 **超标量**
4. 硬件多线程：为系统提供了一种机制，使得当前执行的任务被阻塞时，系统能够继续其他有用的工作
   1. 线程级并行TLP：尝试通过同时执行不同线程来提供并行性。与ILP 相比，TLP 提供的是粗粒度的并行性，即同时执行的程序基本单元 (线程) 比细粒度的程序单元 (单条指令) 更大或者更粗
   2. 细粒度 (fine-grained) 多线程：处理器在每条指令执行完后切换线程，从而跳过被阻塞的线程。尽管这种方法能够避免因为阻塞而导致机器时间的浪费，但它的缺点是，执行很长一段指令的线程在执行每条指令的时候都需要等待
   3. 粗粒度 (coarse-grained) 多线程：为了避免上面细粒度的问题，只切换那些需要等待较长时间才能完成操作 (如从主存中加载) 而被阻塞的线程。这种机制的优点是，不需要线程间的立即切换。但是，处理器还是可能在短阻塞时空闲，线程间的切换也还是会导致延迟
   4. 同步多线程 (SMT) 是细粒度多线程的变种。它通过允许多个线程同时使用多个功能单元来利用超标量处理器的性能

## 3. 并行硬件
1. SIMD 单指令多数据流
   1. 向量处理器：能够对数组或者数据向量进行操作
      1. 向量寄存器：可以存储多个操作组成的向量，并且同时对其内容进行操作
      2. 向量化的功能单元
      3. 向量指令：这些是在向量上操作而不是在标量上操作的指令
      4. 交叉存储器：存储器被分为多个bank，每个memory bank能够独立访问，连续两次访问同一个bank会有一个时间延迟
      5. 步长式存储器访问和硬件散射/聚集。在步长式存储器访问中，程序能够访问向量中固定间隔的元素
   2. GPU
2. MIMD 多指令多数据流
   1. 共享内存系统：一组自治的处理器，通过互连网络 (internection network) 与内存系统相互连接，每个处理器能够访问每个内存区域
      1. 一致内存访问UMA：每个核访问内存中任何一个区域的时间都相同
      2. 非一致内存访问NUMA：访问与核直接连接的那块内存区域比访问其他内存区域要快很多，因为访问其他内存区域需要通过另一块芯片
   2. 分布式内存系统：每个处理器有自己私有的内存空间，处理器-内存对 的之间通过互连网络相互通信
3. Cache一致性
   1. 基于监听的 cache 一致性协议：监听协议的想法来自于基于总线的系统：当多个核共享总线时，总线上传递的信号都能被连接到总线的所有核“看”到
   2. 基于目录的 cache 一致性协议：通过使用一个叫做目录 （directory） 的数据结构来存储每个内存行的状态。一般地，这个数据结构是分布式的

## 4. 并行软件
1. 共享内存
   1. 动态线程：在许多情况下，共享内存程序使用的是动态线程。在这种范式中，有一个主线程，并在任何时刻都有一组工作线程 (可能为空)。主线程通常等待工作请求 (例如，通过网络)，当一个请求到达时，它派生出一个工作线程来执行该请求。当工作线程完成任务，就会终止执行再合并到主线程中。这种模式充分利用了系统的资源，因为线程需要的资源只在线程实际运行时使用
   2. 静态线程：在这种范式中，主线程在完成必需的设置后，派生出所有的线程，在工作结束前所有的线程都在运行。当所有的线程都合并到主线程后，主线程需要做一些清理工作 (如释放内存)，然后也终止
   3. 竞争条件（Race Condition）：是指在多线程或多进程环境下，由于不恰当的执行顺序导致程序的行为出现不确定性或错误的情况。竞争条件通常发生在共享资源的访问和操作过程中，其中两个或多个并发执行的线程或进程尝试同时修改相同的资源，而且执行顺序不受控制，导致不确定性的结果
   4. 临界区：一次只能被一个线程执行的代码块称为临界区
   5. 互斥锁：保证互斥执行的最常用机制是互斥锁，基本思想是每个临界区由一个锁来保护。在一个线程能够执行临界区中的代码前，它必须通过调用一个互斥量函数来获取互斥量，在执行完临界区代码时，通过调用解锁函数来释放互斥量；但是这种方法容易导致 **忙等待**，即想要执行线程的核当无法进入临界区时会重复的检查是否能进入临界区
   6. 信号量 (semaphore) 与互斥量类似，尽管它们的行为细节有些许不同。对某些类型的线程，使用信号量实现同步比用互斥量实现要简单。监视器 (monitor) 能够在更高层次提供互斥执行
2. 分布式内存
   1. 消息传递：消息传递的 API (至少) 要提供一个发送send和一个接收receive函数。进程之间通过它们的序号(rank) 互相识别
   2. 单向通信：单个处理器调用一个函数。在这个函数中，或者用来自另一个进程的值来更新局部内存，或者使用来自于调用进程的值更新远端内存
   
## 5. 性能
1. Amdahl定律
   1. Amdahl 给出了 **固定负载** 下程序并行化效率提升的理论上界
   2. 假设 $P$ 为并行处理器数量， $W = W_S + W_P$ 是问题规模，其中 $W_S$ 是串行分量，$W_P$ 是可并行化部分， $f = W_S / W$ 是串行分量比例， $T_s$ 是串行执行时间， $T_p$ 是并行化后的执行时间， $S$ 是加速比， $E$ 是效率；则一个并行程序的加速比 
      $$
      S = \frac{T_s}{T_p}
      $$
      如果 $S = P$ ，则称该并行程序有 **线性加速比**
      效率
      $$
      E = \frac{S}{P} = \frac{T_s}{P\cdot T_p}
      $$
      Amdahl 定律：在固定负载下，加速公式为
      $$
      \begin{align*}
      S &= \frac{W_S + W_P}{W_S + \frac{W_P}{p}}
      S &= \frac{f + (1 - f)}{f + \frac{1 - f}{p}} = \frac{p}{1 + f(p - 1)} \\
      \lim_{p\to\infty} S &= 1 / f
      \end{align*}
      $$
      可以看到加速比是 **有上限** 的
      另外，考虑并行程序产生的额外开销 $W_O$
      $$
      S = \frac{W_S + W_P}{W_S + \frac{W_P}{p} + W_O} = \frac{p}{1 + f(p - 1) + W_Op/W}
      $$
2. 可扩展性：增加该程序所用的进程/线程数，如果在输入规模也以相应增长率增加的情况下，该程序的效率值一直保持， 那么我们就称该程序是可扩展的

## 6. 并行算法的设计步骤
1. 任务划分（Partitioning）：将整个计算分解为一些小的任务，其目的是尽量开拓并行执行的机会
2. 通信（Communication）分析：分析确定诸任务执行中所需交换的数据和协调诸任务的执行，由此可检测上述任务划分的合理性
3. 任务组合（Agglomeration）：按性能要求和实现的代价来考察前两阶段的结果，必要时可将一些小的任务组合成更大的任务以提高性能或减少通信开销
4. 处理器映射（Mapping）：将每个任务分配到一个处理器上，其目的是最小化全局执行时间和通信成本以及最大化处理器的利用率

